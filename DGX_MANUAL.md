# Etymology AI - Complete User Manual & DGX Deployment Guide

**‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ó‡∏≤‡∏á‡∏£‡∏≤‡∏Å‡∏®‡∏±‡∏û‡∏ó‡πå‡∏î‡πâ‡∏ß‡∏¢ Deep Learning**

---

## üìö ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. [‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö](#1-‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö)
2. [‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°](#2-‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°)
3. [‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•](#3-‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
4. [‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•](#4-‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•)
5. [‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô DGX](#5-‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô-dgx)
6. [‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß](#6-‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß)
7. [‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤](#7-‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤)

---

## 1. ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö

### 1.1 ‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ Deep Learning ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£:
- **‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ phonetic embeddings** ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤
- **‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö cognates** (‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏Å‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
- **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå** ‡∏ó‡∏≤‡∏á‡∏£‡∏≤‡∏Å‡∏®‡∏±‡∏û‡∏ó‡πå

### 1.2 ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏£‡∏£‡∏£‡∏°
```
[Input Words] ‚Üí [IPA Conversion] ‚Üí [Phonetic Embedding]
                                          ‚Üì
                                    [Siamese Network] ‚Üí [Cognate Detection]
                                          ‚Üì
                                    [Etymology GNN] ‚Üí [Relationship Graph]
```

### 1.3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å
1. **Phonetic Embedding Model**: Transformer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á IPA ‡πÄ‡∏õ‡πá‡∏ô vector
2. **Siamese Network**: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô cognates ‡∏î‡πâ‡∏ß‡∏¢ triplet loss
3. **Etymology GNN**: Graph Attention Network ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå

---

## 2. ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°

### 2.1 ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ (Local)**:
- Python 3.9+ (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ 3.10)
- GPU: NVIDIA GPU with 16GB+ VRAM
- RAM: 32GB+
- Disk: 50GB free space

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏ö‡∏ô DGX**:
- DGX A100 (8x GPUs)
- SLURM workload manager
- Shared filesystem

### 2.2 ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies

```bash
cd /home/67070309/eak_project/etymology_ai

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Conda environment (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
conda create -n etymology python=3.10
conda activate etymology

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyTorch (CUDA 11.8)
conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á torch-geometric
pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu118.html

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
pip install pytorch-lightning wandb tensorboard
pip install pythainlp epitran panphon
pip install pandas numpy pyyaml tqdm requests beautifulsoup4
```

### 2.3 ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå

```
etymology_ai/
‚îú‚îÄ‚îÄ configs/                    # ‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
‚îÇ   ‚îú‚îÄ‚îÄ data_sources.yaml       
‚îÇ   ‚îî‚îÄ‚îÄ model_config.yaml       
‚îú‚îÄ‚îÄ data/                       
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö
‚îÇ   ‚îî‚îÄ‚îÄ processed/              # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏ì‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/                   # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py          # PyTorch Dataset classes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ phonetic_converter.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.py
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # ‡πÇ‡∏°‡πÄ‡∏î‡∏• Neural Network
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ phonetic_embedding.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ siamese_network.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ etymology_gnn.py
‚îÇ   ‚îú‚îÄ‚îÄ training/               # ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡πÄ‡∏ó‡∏£‡∏ô
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_phonetic_embedding.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_siamese.py
‚îÇ   ‚îî‚îÄ‚îÄ inference/              # ‡πÇ‡∏Ñ‡πâ‡∏î inference
‚îÇ       ‚îî‚îÄ‚îÄ cognate_predictor.py
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ slurm/                  # SLURM job scripts ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DGX
‚îÇ       ‚îú‚îÄ‚îÄ train_phonetic.slurm
‚îÇ       ‚îî‚îÄ‚îÄ train_siamese.slurm
‚îî‚îÄ‚îÄ outputs/                    # Output ‡πÅ‡∏•‡∏∞ checkpoints
```

---

## 3. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

### 3.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á sample data (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö)
python src/data/sample_dataset.py
```

### 3.2 ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á (Optional)

```bash
# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Kaikki Thai etymology data
python scripts/download_sample_data.py --source kaikki --language thai

# ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
python src/data/preprocessor.py
```

### 3.3 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```python
from src.data.dataset import CognateDataset
from src.data.phonetic_converter import PhoneticConverter

converter = PhoneticConverter()
dataset = CognateDataset(
    "data/raw/sample_etymology_data.json",
    converter,
    mode="triplet"
)

print(f"Dataset size: {len(dataset)}")
sample = dataset[0]
print(f"Sample: {sample['anchor_text']} - {sample['positive_text']}")
```

---

## 4. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•

### 4.1 ‡πÄ‡∏ó‡∏£‡∏ô Phonetic Embedding (‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1)

**Local (Single GPU)**:
```bash
python src/training/train_phonetic_embedding.py \
    --config configs/model_config.yaml \
    --data data/raw/sample_etymology_data.json \
    --output outputs/phonetic_embedding \
    --epochs 50 \
    --batch-size 32
```

**DGX (8 GPUs)** - ‡∏î‡∏π‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5

### 4.2 ‡πÄ‡∏ó‡∏£‡∏ô Siamese Network (‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2)

```bash
python src/training/train_siamese.py \
    --config configs/model_config.yaml \
    --data data/raw/sample_etymology_data.json \
    --encoder outputs/phonetic_embedding/checkpoints/best.ckpt \
    --output outputs/siamese
```

### 4.3 ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô

```bash
# ‡πÄ‡∏õ‡∏¥‡∏î TensorBoard
tensorboard --logdir outputs/

# ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÑ‡∏õ‡∏ó‡∏µ‡πà http://localhost:6006
```

---

## 5. ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô DGX

### 5.1 ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö DGX

```bash
# SSH ‡πÄ‡∏Ç‡πâ‡∏≤ DGX
ssh username@dgx-server-ip

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPUs
nvidia-smi

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö SLURM
sinfo
```

### 5.2 ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment ‡∏ö‡∏ô DGX

```bash
# ‡πÇ‡∏Ñ‡∏•‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå
cd /shared/username/
git clone https://github.com/Eakkachad/etymology_ai.git
cd etymology_ai

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Conda environment
conda create -n etymology python=3.10
conda activate etymology

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies (‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô 2.2)
```

### 5.3 Submit SLURM Jobs

#### ‡πÄ‡∏ó‡∏£‡∏ô Phonetic Embedding

```bash
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç train_phonetic.slurm ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
nano scripts/slurm/train_phonetic.slurm

# Submit job
sbatch scripts/slurm/train_phonetic.slurm

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
squeue -u $USER

# ‡∏î‡∏π log
tail -f outputs/logs/phonetic_*.out
```

#### ‡πÄ‡∏ó‡∏£‡∏ô Siamese Network

```bash
# ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å phonetic embedding ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à
sbatch scripts/slurm/train_siamese.slurm
```

### 5.4 ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Jobs

```bash
# ‡∏î‡∏π‡∏Ñ‡∏¥‡∏ß‡∏á‡∏≤‡∏ô
squeue

# ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å job
scancel <job_id>

# ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î job
scontrol show job <job_id>

# ‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
sacct -u $USER
```

### 5.5 ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Multi-GPU

‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ä‡πâ PyTorch Lightning DDP ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥:
- `devices=8` ‡πÉ‡∏ô config = ‡πÉ‡∏ä‡πâ 8 GPUs
- `strategy="ddp"` = Distributed Data Parallel
- Automatic gradient synchronization
- Linear scaling of batch size

---

## 6. ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß

### 6.1 Cognate Detection

```python
from src.inference.cognate_predictor import CognatePredictor

# Load trained model
predictor = CognatePredictor('outputs/siamese/checkpoints/best.ckpt')

# Predict cognate score
score = predictor.predict(
    word1="mƒÅt·πõ",   lang1="san",
    word2="mother", lang2="eng"
)
print(f"Cognate probability: {score:.3f}")

# Find cognates from candidate list
candidates = ["mother", "father", "brother", "water", "fire"]
cognates = predictor.find_cognates(
    query_word="mƒÅt·πõ",
    query_lang="san",
    candidate_words=candidates,
    candidate_lang="eng",
    threshold=0.7
)
print(f"Cognates: {cognates}")
```

### 6.2 Phonetic Embedding Extraction

```python
from src.models.phonetic_embedding import PhoneticEmbeddingModel
import torch

model = PhoneticEmbeddingModel.load_from_checkpoint(
    'outputs/phonetic_embedding/checkpoints/best.ckpt'
)
model.eval()

# Encode word
word_ipa = "maÀêtrÃ©"
codes = [ord(c) for c in word_ipa[:50]]
codes += [0] * (50 - len(codes))
x = torch.tensor([codes], dtype=torch.long)

with torch.no_grad():
    embedding = model(x)

print(f"Embedding shape: {embedding.shape}")  # (1, 512)
```

---

## 7. ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤

### 7.1 Out of Memory (OOM)

**‡∏õ‡∏±‡∏ç‡∏´‡∏≤**: GPU memory ‡πÄ‡∏ï‡πá‡∏°

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**:
```bash
# ‡∏•‡∏î batch size
python src/training/train_phonetic_embedding.py --batch-size 128

# ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° gradient accumulation
# ‡πÅ‡∏Å‡πâ‡πÉ‡∏ô configs/model_config.yaml:
# accumulate_grad_batches: 4
```

### 7.2 SLURM Job Failed

```bash
# ‡∏î‡∏π error log
cat outputs/logs/phonetic_<job_id>.err

# ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:
# 1. CUDA version mismatch ‚Üí ‡πÅ‡∏Å‡πâ module load cuda 
# 2. Conda environment ‡πÑ‡∏°‡πà active ‚Üí ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö SLURM script
# 3. Path ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‚Üí ‡πÉ‡∏ä‡πâ absolute paths
```

### 7.3 pythainlp ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ

```bash
# ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Python 3.9+
conda install python=3.10

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á pythainlp ‡πÉ‡∏´‡∏°‡πà
pip install pythainlp==5.0.0
```

### 7.4 torch-geometric ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ

```bash
# ‡πÉ‡∏ä‡πâ conda (‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤)
conda install pyg -c pyg

# ‡∏´‡∏£‡∏∑‡∏≠ pip ‡πÅ‡∏ö‡∏ö‡∏£‡∏∞‡∏ö‡∏∏ CUDA version
pip install torch-geometric torch-scatter torch-sparse \
    -f https://data.pyg.org/whl/torch-2.1.0+cu118.html
```

---

## 8. Performance Tuning

###8.1 Multi-GPU Scaling

**Expected speedup**:
- 1 GPU: ~100 samples/sec
- 4 GPUs: ~380 samples/sec (3.8x)
- 8 GPUs: ~720 samples/sec (7.2x)

**Tips**:
- ‡πÉ‡∏ä‡πâ `num_workers=4` ‡πÉ‡∏ô DataLoader
- Enable `pin_memory=True`
- ‡πÉ‡∏ä‡πâ `precision="16-mixed"` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mixed precision training

### 8.2 Monitoring

```bash
# GPU utilization
watch -n 1 nvidia-smi

# Training progress
tensorboard --logdir outputs/

# System resources
htop
```

---

## 9. ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢ (FAQ)

**Q: ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?**
A: ‡πÉ‡∏ä‡πà ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö: Phonetic Embedding ‚Üí Siamese Network ‚Üí GNN

**Q: ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?**
A: ‡∏ö‡∏ô DGX A100 (8 GPUs):
- Phonetic Embedding: ~3-4 hours (100 epochs)
- Siamese Network: ~2-3 hours

**Q: ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ CPU ‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?**
A: ‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å (~50-100x) ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dataset ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà

**Q: checkpoint files ‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?**
A:
- Phonetic Embedding: ~200-300 MB
- Siamese Network: ~250-350 MB
- GNN: ~150-200 MB

---

## 10. ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡πà‡∏≠

### 10.1 Adding New Languages

1. ‡πÄ‡∏û‡∏¥‡πà‡∏° config ‡πÉ‡∏ô `configs/data_sources.yaml`
2. Implement IPA conversion ‡πÉ‡∏ô `phonetic_converter.py`
3. ‡πÄ‡∏û‡∏¥‡πà‡∏° language family encoding

### 10.2 Custom Models

‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô `src/models/` ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á class ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∑‡∏ö‡∏ó‡∏≠‡∏î‡∏à‡∏≤‡∏Å `nn.Module`

### 10.3 Hyperparameter Tuning

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç configs/model_config.yaml`:
- learning_rate
- batch_size
- num_layers, num_heads
- dropout

---

‡∏ú‡∏π‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤: Eakkachad & Antigravity AI Assistant
‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 2026-02-07
